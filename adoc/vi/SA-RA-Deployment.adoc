
== Deployment

FixMe - Varius sit amet mattis vulputate. Nisi scelerisque eu ultrices vitae auctor eu augue ut. Integer vitae justo eget magna fermentum iaculis eu non diam. Rhoncus urna neque viverra justo. Elementum tempus egestas sed sed risus. Porta nibh venenatis cras sed felis eget velit aliquet sagittis. Venenatis a condimentum vitae sapien pellentesque. Magna ac placerat vestibulum lectus mauris ultrices eros in cursus. Nibh cras pulvinar mattis nunc. Tempor orci dapibus ultrices in iaculis nunc. Sapien nec sagittis aliquam malesuada bibendum arcu vitae elementum. Nisi porta lorem mollis aliquam. Laoreet id donec ultrices tincidunt arcu non sodales.

////
The following typographical conventions are used in this manual:
    • /etc/passwd: directory names and file names
    • placeholder: replace placeholder with the actual value
    • PATH: the environment variable PATH
    • Alt, Alt+F1: a key to press or a key combination; keys are shown in uppercase as on a keyboard
    • File, File > Save As: menu items, buttons
    • command line: commands executed in the console
////

=== Deployment overview

FixMe - Add simplistic drawing showing stack and user perspective

ifdef::HWDepCfg[]

=== Hardware deployment configuration

ifdef::iRancher[]
ifdef::d1[]

Leveraging the enterprise grade of the following software components being deployed, many hardware platforms are enabled and can readily be used.

include::../SUSE/YES.adoc[]

Further, by reviewing the minimum requirements for each of the software layers, in conjunction with the target solution deployment context, the processor, memory, disk and networking aspects can be determined.

endif::d1[]
endif::iRancher[]

endif::HWDepCfg[]

ifdef::SWDepCfg[]

=== Software deployment configuration

ifdef::iRancher[]
ifdef::d1[]

include::../SUSE/SA.adoc[]

By utilizing three products from the {companyName} portfolio:

ifdef::iSLEMicro[]
* Operating System - {pn_SLEMicro}
endif::iSLEMicro[]
ifdef::iK3s[]
* Kubernetes - {pn_K3s}
endif::iK3s[]
* Multi-cluster Management - {pn_Rancher}

one can build the necessary infrastructure and service to administer and manage multiple Kubernets clusters. These layered components are described in the following sections.

endif::d1[]
endif::iRancher[]

==== Operating System Deployment

ifdef::iRancher[]
ifdef::d1[]

include::../SUSE/SLE-Micro/SA.adoc[]

To accomodate many options of underlying physical or virtualization hosts, this operating system supports a vast breadth of platforms and component resources. Further, it provides the core functionality for the next layer of Kubernetes-based infrastructure software.

The installation process is described and can be performed, per the product documentation, by following:

* the {pn_SLE-Micro_InstallationDocURL}[Installation Quick Start] for
** manual installation
** raw image deployment
* or {pn_SLE-Micro_AutoYaSTDocURL}[AutoYaST Guide] for unnatttended installations

TIP:: An additional consideration is, for the first node deployed, to create an additional IP address on the host network interface card. This can be used for the {pn_Rancher} access, which may also become managed by a load-balancer if a multi-node cluster becomes the base.

endif::d1[]
endif::iRancher[]

ifdef::iRancher[]
ifdef::d1[]
==== Kubernetes Deployment

This design leverage the {k8sName} Kubernetes distribution. {k8sName} is a highly available, CNCF certified Kubernetes distribution capable of deploying any Kubernetes production workload. {k8sName} is packaged as a single binary with minimal software dependencies. This signficantly reduces the expertise and effort required install, run, and maintain a production ready Kubernetes cluster.

For this deployment, a single server installed with the {osName} immutable operating system will support a single instance of {k8sName}. For maximum flexibility, {k8sName} will be deployed in a manner that would allow expanding the single-node cluster into a highly available, three-node Kubernetes cluster at a later date. 

While it is highly recommended that Kubernetes workloads (in this case the {pn_Rancher} ) be isolated from the Kubernetes control-plane and data-plane; this design will maintain all functions, including the {porfolioName} Server, on the server node. In this specialized case, the {pn_Rancher} workload is a known quantity and no other workloads will be run on this Kubernetes cluster. For this reason the {pn_Rancher} cluster is more closely aligned with appliance model best practices.

.The primary steps for deploying this single node {k8sName} cluster are:
1. (Optional) Provide the server with one extra IP address that will be used as the primary address for accessing the {k8sName} cluster API server. This will allow the cluster to grown beyond a single server node. It is not neccessary if there will be an external load balancer used to access the cluster, or if the cluster will never be grown beyond a single server node.
2. Find the appropriate version of the {k8sName} binary
3. Install {k8sName} with embedded etcd enabled

.Step 1: (Optional) Provide the server with one extra IP address:
* If needed, use the `ip a` command to determine the netmask for the second IP address, based on the CIDR notation for eth0
* Set the following variable with the IP address and CIDR notation that will be used to access the Kubernetes API server:
----
SECOND_IP=""
----
** For example: `SECOND_IP="10.111.2.100/24"`

----
sudo cp -np /etc/sysconfig/network/ifcfg-eth0 ~/ifcfg-eth0.`date +"%d.%b.%Y.%H.%M"`
cp -p ~/ifcfg-eth0.`date +"%d.%b.%Y"`* ~/ifcfg-eth0
echo "IPADDR_2=${SECOND_IP}" >> ~/ifcfg-eth0
diff /etc/sysconfig/network/ifcfg-eth0 ~/ifcfg-eth0
----
* Ensure the only difference between the original ifcfg-eth0 file and the updated ~/ifcfg-eth0 is the extra "IPADDR_2" line, then run the following commands:
----
sudo mv ~/ifcfg-eth0 /etc/sysconfig/network/ifcfg-eth0
sudo systemctl restart network.service
ip a
----
* The original server IP address and the additional IP address should be shown with the correct CIDR notation


.Step 2: Find the appropriate version of the {k8sName} binary: 
* At the time of writing, the most current, supported version of {k8sName} for {pn_Rancher} is v1.20.4+k3s1. Verify the supported versions at: https://rancher.com/support-maintenance-terms/, under the "Rancher Support Matrix"
* Set the following variable with the desired version of {k8sName}

----
K3s_VERSION=""
----
* I.e. `K3s_VERSION="v1.20.4+k3s1"`


.Step 3: Install {k8sName} with embedded etcd enabled:
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644' sh -s -
----
* Monitor the progress of the installation: `watch -c "kubectl get pods -n kube-system"`
** The installation is complete when all pods have a status of "Completed" or a status of "Running" with the number of "READY" pods being "1/1", "2/2", etc.
** Use Ctrl+c to exit the watch loop after all pods are running

////
Procedure for adding servers. FIRST_SERVER_IP should be the extra IP, if configured, of the first K3s server
----
FIRST_SERVER_IP=""
sudo K3S_TOKEN=<from the /var/lib/rancher/k3s/server/node-token file on the first server> INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --server https://${FIRST_SERVER_IP}:6443 --write-kubeconfig-mode=644'  ./install.sh
----
////

endif::d1[]
endif::iRancher[]

ifdef::iRancher[]
ifdef::d1[]

==== {pn_Rancher} Deployment

{pn_Rancher} is a complete solution for managing Kubernetes clusters and Kubenetes applications. It addresses the operational and security challenges of managing multiple Kubernetes clusters and applications across any infrastructure. {pn_Rancher} streamlines Kubernetes cluster management on bare metal servers, private clouds, and vSphere environements; from the datacenter to the edge. {pn_Rancher} unites all of your Kubernetes clusters with global security policies, centralized authentication, access control and observability.

As {pn_Rancher} server is a native Kubernetes application, it will run on the single-node {k8sName} cluster. In instances where a load balancer is used to support the {k8sName} cluster, deploying two additional {k8sName} cluster nodes will automatically make {pn_Rancher} highly available. {pn_Rancher} uses the {k8sName} etcd key/value store to persist its data, which offers several advantages. Providing highly-available storage isn't needed to make {pn_Rancher} highly available. In addition, backing up the {k8sName} etcd store protects the cluster as well as the installation of {pn_Rancher}.

ifdef::iK3s[]
NOTE: These deployment steps are specific to {pn_K3s}. They can be executed from any host or node that has the kubectl tool and the KUBECONFIG file for the {k8sName} cluster.

The steps described here are for deploying {pn_Rancher} with self-signed security certificates. Other options are to have {pn_Rancher} create public certificates via Let's Encrypt (only with a publicly resolvable hostname for the {pn_Rancher} server) and to provide preconfigured, private certificates. See https://rancher.com/docs/rancher/v2.x/en/installation/install-rancher-on-k8s/#3-choose-your-ssl-configuration for more information.

.The primary steps for deploying {pn_Rancher} are:
1. Create the HelmChart custom resource for cert-manager
2. Create the HelmChart custom resource for {pn_Rancher}
3. Expose {pn_Rancher} through a Kubernetes NodePort service
4. (Optional) Create an SSH tunnel to access {pn_Rancher} in cases where the exposed server IP address and/or port is not accessible to the client browser
5. Connect to the {pn_Rancher} web UI

.Step 1: Create the HelmChart custom resource for cert-manager:
* At the time of writing, the most current, supported version of cert-manager is v1.0.4
* Set the following variable with the desired version of cert-manager
----
CERT_MANAGER_VERSION=""
----
* I.e. `CERT_MANAGER_VERSION="v1.0.4"`

* Create the cert-manager HelmChart custom resource manifest
----
cat <<EOF> cert-manager-helm-crd.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: kube-system
spec:
  chart: cert-manager
  targetNamespace: cert-manager
  version: ${CERT_MANAGER_VERSION}
  repo: https://charts.jetstack.io
EOF
----

* Create the cert-manager CRDs and apply the HelmChart resource manifest: 
----
kubectl create namespace cert-manager
kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/${CERT_MANAGER_VERSION}/cert-manager.crds.yaml
sudo mv cert-manager-helm-crd.yaml /var/lib/rancher/k3s/server/manifests/
----

* Monitor the progress of the installation: `watch -c "kubectl get pods -n cert-manager"`
** The installation is complete when all pods have a status of "Completed" or a status of "Running" with the number of "READY" pods being "1/1", "2/2", etc.
** Use Ctrl+c to exit the watch loop after all pods are running

.Step 2: Create the HelmChart custom resource for {pn_Rancher}:
* Set the following variable to the hostname of the {pn_Rancher} server instance
----
HOSTNAME=""
----
* I.e. `HOSTNAME="suse-rancher.sandbox.local"`

* Create the SUSE-Rancher HelmChart custom resource manifest
----
cat <<EOF> suse-rancher-helm-crd.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: rancher
  namespace: kube-system
spec:
  chart: rancher
  targetNamespace: cattle-system
  repo: https://releases.rancher.com/server-charts/stable
  set:
    hostname: ${HOSTNAME}
EOF
----

* Apply the HelmChart resource manifest: 
----
kubectl create namespace cattle-system
sudo mv suse-rancher-helm-crd.yaml /var/lib/rancher/k3s/server/manifests/
----

* Monitor the progress of the installation: `watch -c "kubectl get pods -n cattle-system"`
** The installation is complete when all pods have a status of "Completed" or a status of "Running" with the number of "READY" pods being "1/1", "2/2", etc.
** Use Ctrl+c to exit the watch loop after all pods are running

.Step 3: Expose {pn_Rancher} through a Kubernetes NodePort service
* Verify that the {pn_Rancher} service is currently of the type ClusterIP:
----
kubectl -n cattle-system get svc/rancher
----
* If the {pn_Rancher} service is ClusterIP, use the following commands to change it to NodePort and assign it port 30443 on the Linux host:

----
cat <<EOF> patch-NodePort.yaml
spec:
  type: NodePort
  ports:
    - port: 443
      nodePort: 30443
      name: https-internal
EOF
----
----
kubectl patch -n cattle-system svc/rancher --patch "$(cat patch-NodePort.yaml)"
----
* Verify the exposed port for the {pn_Rancher} service:
----
kubectl -n cattle-system get svc/rancher | awk -F443: '{print$2}' | awk -F\/ '{print$1}'
----

.Step 4: (Optional) Create an SSH tunnel to access {pn_Rancher}: 
 
 NOTE: This optional step is useful in cases where NAT routers and/or firewalls prevent the client web browser from reaching the exposed {pn_Rancher} server IP address and/or port. This step requires that a Linux host is accessible through SSH from the client system and that the Linux host can reach the exposed {pn_Rancher} server IP address and port.

* Create an SSH tunnel through the Linux host to the IP address of the {pn_Rancher} server on the NodePort, as noted in Step 3:
----
ssh -L localhost:<NodePort>:<IP of {pn_Rancher}>:<NodePort> -N user@Linux-host
----
** The SSH tunnel syntax rules require a combination of localhost/port and remote-host/port per -L switch. Use multiple -L switches to expose more than one remote-host/port.
** -N tells SSH that it won't send any commands after establishing connection


.Step 5: Connect to the {pn_Rancher} web UI and configure {pn_Rancher}:
* On the client system, use a web browser to connect to the {pn_Rancher} server on exposed NodePort 
** I.e. `https://172.16.230.10:NodePort` or (with the SSH tunnel) `https://localhost:NodePort`
* Provide a new Admin password
* IMPORTANT: On the second configuration page, ensure the "Rancher Server URL" is set to the second IP address configured on the Linux host (or the load balancer IP address, if available) and the correct NodePort

** I.e. `10.111.2.110:30443`

endif::iK3s[]

endif::d1[]
endif::iRancher[]

endif::SWDepCfg[]
