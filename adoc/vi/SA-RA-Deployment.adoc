
== Deployment

FixMe - Varius sit amet mattis vulputate. Nisi scelerisque eu ultrices vitae auctor eu augue ut. Integer vitae justo eget magna fermentum iaculis eu non diam. Rhoncus urna neque viverra justo. Elementum tempus egestas sed sed risus. Porta nibh venenatis cras sed felis eget velit aliquet sagittis. Venenatis a condimentum vitae sapien pellentesque. Magna ac placerat vestibulum lectus mauris ultrices eros in cursus. Nibh cras pulvinar mattis nunc. Tempor orci dapibus ultrices in iaculis nunc. Sapien nec sagittis aliquam malesuada bibendum arcu vitae elementum. Nisi porta lorem mollis aliquam. Laoreet id donec ultrices tincidunt arcu non sodales.

////
The following typographical conventions are used in this manual:
    • /etc/passwd: directory names and file names
    • placeholder: replace placeholder with the actual value
    • PATH: the environment variable PATH
    • Alt, Alt+F1: a key to press or a key combination; keys are shown in uppercase as on a keyboard
    • File, File > Save As: menu items, buttons
    • command line: commands executed in the console
////

=== Deployment overview

FixMe - Add simplistic drawing showing stack and user perspective

ifdef::HWDepCfg[]

=== Hardware deployment configuration

ifdef::pnRancher[]
ifdef::pnRancherPilot[]

Leveraging the enterprise grade of the following software components being deployed, many hardware platforms are enabled and can readily be used.

include::../SUSE/YES.adoc[]

Further, by reviewing the minimum requirements for each of the software layers, in conjunction with the target solution deployment context, the processor, memory, disk and networking aspects can be determined.

endif::pnRancherPilot[]
endif::pnRancher[]

endif::HWDepCfg[]

ifdef::SWDepCfg[]

=== Software deployment configuration

ifdef::pnRancher[]
ifdef::pnRancherPilot[]

include::../SUSE/SA.adoc[]

By utilizing three products from the {companyName} portfolio:

* Operating System - {osName}
* Kubernetes - {k8sName}
* Multi-cluster Management - {portfolioName}

one can build the necessary infrastructure and service to administer and manage multiple Kubernets clusters. These layered components are described in the following sections.

endif::pnRancherPilot[]
endif::pnRancher[]

==== Operating System Deployment

ifdef::pnRancher[]
ifdef::pnRancherPilot[]

include::../SUSE/SLE-Micro/SA.adoc[]

To accomodate many options of underlying physical or virtualization hosts, this operating system supports a vast breadth of platforms and component resources. Further, it provides the core functionality for the next layer of Kubernetes-based infrastructure software.

The installation process is described and can be performed, per the product documentation, by following:

* the {pn_SLE-Micro_InstallationDocURL}[Installation Quick Start] for
** manual installation
** raw image deployment
* or {pn_SLE-Micro_AutoYaSTDocURL}[AutoYaST Guide] for unnatttended installations

TIP:: An additional consideration is, for the first node deployed, to create an additional IP address on the host network interface card. This can be used for the {portfolioName} access, which may also become managed by a load-balancer if a multi-node cluster becomes the base.

endif::pnRancherPilot[]
endif::pnRancher[]

ifdef::pnRancher[]
ifdef::pnRancherPilot[]
==== Kubernetes Deployment

This design leverage the {k8sName} Kubernetes distribution. {k8sName} is a highly available, CNCF certified Kubernetes distribution capable of deploying any Kubernetes production workload. {k8sName} is packaged as a single binary with minimal software dependencies. This signficantly reduces the expertise and effort required install, run, and maintain a production ready Kubernetes cluster.

For this deployment, a single server installed with the {osName} immutable operating system will support a single instance of {k8sName}. For maximum flexibility, {k8sName} will be deployed in a manner that would allow expanding the single-node cluster into a highly available, three-node Kubernetes cluster. This will be only slightly more complex that the one-line, 45 second (curl -sfL https://get.k3s.io | sh -) minimal installation method of {k8sName}.

While it is highly recommended that Kubernetes workloads (in this case the {portfolioName} Server) be isolated from the Kubernetes control-plane and data-plane; this design will maintain all functions, including the {porfolioName} Server, on the server node. In this specialized case, the {portfolioName} Server workload is a known quantity and no other workloads will be run on this Kubernetes cluster. For this reason the {portfolioName} Server cluster is more closely aligned with appliance model best practices.

At the time of writing, the most current, supported version of {k8sName} for {portfolioName} Server is 1.20.4. Verify the supported versions at: https://rancher.com/support-maintenance-terms/

The primary steps for deploying this three node {k8sName} cluster are:
1. Ensure the first server has one extra IP address configured. This will be used as the primary address for accessing the {k8sName} API server.
2. Download the appropriate version of the {k8sName} binary from: https://github.com/rancher/k3s/releases/latest
3. Download the installation script from: https://get.k3s.io
4. Install {k8sName} with etcd enabled

////
To deploy the first server and add a second server node to the cluster:
* Deploy {k8sName} with --cluster-init to enable embedded etcd:
----
wget https://github.com/k3s-io/k3s/releases/download/v1.20.4%2Bk3s1/k3s
sudo mv k3s /usr/local/bin/
sudo chmod 755 /usr/local/bin/k3s
curl -sfL https://get.k3s.io > install.sh
chmod 755 install.sh
INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644'  ./install.sh
----

----
FIRST_SERVER_IP=""
sudo K3S_TOKEN=<from the /var/lib/rancher/k3s/server/node-token file on the first server> INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --server https://${FIRST_SERVER_IP}:6443 --write-kubeconfig-mode=644'  ./install.sh
----
////

endif::pnRancherPilot[]
endif::pnRancher[]

ifdef::pnRancher[]
ifdef::pnRancherPilot[]

==== {portfolioName} Deployment

{portfolioName} server is a complete solution for managing Kubernetes clusters and Kubenetes applications. It addresses the operational and security challenges of managing multiple Kubernetes clusters and applications across any infrastructure. {portfolioName} streamlines Kubernetes cluster management on bare metal servers, private clouds, and vSphere environements; from the datacenter to the edge. {portfolioName} unites all of your Kubernetes clusters with global security policies, centralized authentication, access control and observability.

As {portfolioName} server is a native Kubernetes application, it will run on the single-node {k8sName} cluster. In instances where a load balancer is used to support the {k8sName} cluster, deploying two additional {k8sName} cluster nodes will automatically make {portfolioName} server highly available. {portfolioName} server uses the {k8sName} etcd key/value store to persist its data, which offers several advantages. Additional, highly available, storage isn't needed to make {portfolioName} server highly available. In addition, backing up the {k8sName} etcd store protects the cluster as well as the installation of {portfolioName} server.

endif::pnRancherPilot[]
endif::pnRancher[]

endif::SWDepCfg[]
