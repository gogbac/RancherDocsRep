
== Deployment

FixMe - add overview

////
The following typographical conventions are used in this manual:
    • /etc/passwd: directory names and file names
    • placeholder: replace placeholder with the actual value
    • PATH: the environment variable PATH
    • Alt, Alt+F1: a key to press or a key combination; keys are shown in uppercase as on a keyboard
    • File, File > Save As: menu items, buttons
    • command line: commands executed in the console
////

=== Deployment overview

FixMe - Add simplistic drawing showing stack and user perspective

ifdef::HWDepCfg[]

==== Hardware deployment configuration

ifdef::iRancher[]
ifdef::d1[]

FixMe - cite required network infrastructure

FixMe - cite host/node BMC/CPU/Mem/Disk/Nework prep

endif::d1[]
endif::iRancher[]

endif::HWDepCfg[]

ifdef::SWDepCfg[]

==== Operating System Deployment

ifdef::iRancher[]
ifdef::d1[]

include::../SUSE/SLE-Micro/SA_vars.adoc[]
include::../SUSE/Manager/SA_vars.adoc[]
include::../SUSE/RMT/SA_vars.adoc[]

Core Infrastructure Components / Services::
** Domain Name Service (DNS) - an external network-accessible service to map IP Addresses to hostnames
** Network Time Protocol (NTP) - an external network-accessible service to obtain and synchronize system times to aid in timestamp consistency
** Software Update Service - access to a network-based repository for software update packages. This can be accessed directly from each node via registration to
*** the general, internet-based {suseSCCPage}[{companyName} Customer Center] ( SCC ) or
*** an organization's {pn_SUMa_ProductPage}[{pn_SUMa}] or
*** a local server running an instance of {pn_RMT_DocURL}[{pn_RMT}] ( {an_RMT} )
+
NOTE: As each node is deployed, it can be pointed to the respective update service and update notification and applicate will be managed by the configuration management web interface.

Process::
Follow these steps
* Download the {pn_SLEMicro_Download}[{pn_SLEMicro}] product (either for the ISO or Virtual Machine image)
* The installation process is described and can be performed with default valuesaside from your local network addressing, per the product documentation. Simply follow:
** the {pn_SLEMicro_InstallationDocURL}[Installation Quick Start] for
*** manual installation
*** raw image deployment
** or {pn_SLEMicro_AutoYaSTDocURL}[AutoYaST Guide] for unnatttended installations
+
TIP: An additional consideration is, for the first node deployed, to create an additional IP address on the host network interface card. This can be used for the {pn_Rancher} access, which may also become managed by a load-balancer if a multi-node cluster becomes the base.

endif::d1[]
endif::iRancher[]

ifdef::iRancher[]
ifdef::d1[]

==== Kubernetes Deployment

This design leverage the {pn_K3s} Kubernetes distribution. {pn_K3s} is a highly available, CNCF certified Kubernetes distribution capable of deploying any Kubernetes production workload. {pn_K3s} is packaged as a single binary with minimal software dependencies. This signficantly reduces the expertise and effort required install, run, and maintain a production ready Kubernetes cluster.

For this deployment, a single server installed with the {pn_SLEMicro} immutable operating system will support a single instance of {pn_K3s}. For maximum flexibility, {pn_K3s} will be deployed in a manner that would allow expanding the single-node cluster into a highly available, three-node Kubernetes cluster at a later date. 

While it is highly recommended that Kubernetes workloads (in this case the {pn_Rancher} ) be isolated from the Kubernetes control-plane and data-plane; this design will maintain all functions, including the {porfolioName} Server, on the server node. In this specialized case, the {pn_Rancher} workload is a known quantity and no other workloads will be run on this Kubernetes cluster. For this reason the {pn_Rancher} cluster is more closely aligned with appliance model best practices.

.The primary steps for deploying this single node {pn_K3s} cluster are:
1. (Optional) Provide the server with one extra IP address that will be used as the primary address for accessing the {pn_K3s} cluster API server. This will allow the cluster to grown beyond a single server node. It is not neccessary if there will be an external load balancer used to access the cluster, or if the cluster will never be grown beyond a single server node.
2. Find the appropriate version of the {pn_K3s} binary
3. Install {pn_K3s} with embedded etcd enabled

.Step 1: (Optional) Provide the server with one extra IP address:
* If needed, use the `ip a` command to determine the netmask for the second IP address, based on the CIDR notation for eth0
* Set the following variable with the IP address and CIDR notation that will be used to access the Kubernetes API server:
----
SECOND_IP=""
----
** For example: `SECOND_IP="10.111.2.100/24"`

----
sudo cp -np /etc/sysconfig/network/ifcfg-eth0 ~/ifcfg-eth0.`date +"%d.%b.%Y.%H.%M"`
cp -p ~/ifcfg-eth0.`date +"%d.%b.%Y"`* ~/ifcfg-eth0
echo "IPADDR_2=${SECOND_IP}" >> ~/ifcfg-eth0
diff /etc/sysconfig/network/ifcfg-eth0 ~/ifcfg-eth0
----
* Ensure the only difference between the original ifcfg-eth0 file and the updated ~/ifcfg-eth0 is the extra "IPADDR_2" line, then run the following commands:
----
sudo mv ~/ifcfg-eth0 /etc/sysconfig/network/ifcfg-eth0
sudo systemctl restart network.service
ip a
----
* The original server IP address and the additional IP address should be shown with the correct CIDR notation


.Step 2: Find the appropriate version of the {pn_K3s} binary: 
* At the time of writing, the most current, supported version of {pn_K3s} for {pn_Rancher} is v1.20.4+k3s1. Verify the supported versions at: https://rancher.com/support-maintenance-terms/, under the "Rancher Support Matrix"
* Set the following variable with the desired version of {pn_K3s}

----
K3s_VERSION=""
----
* I.e. `K3s_VERSION="v1.20.4+k3s1"`


.Step 3: Install {pn_K3s} with embedded etcd enabled:
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644' sh -s -
----
* Monitor the progress of the installation: `watch -c "kubectl get pods -n kube-system"`
** The installation is complete when all pods have a status of "Completed" or a status of "Running" with the number of "READY" pods being "1/1", "2/2", etc.
** Use Ctrl+c to exit the watch loop after all pods are running

////
Procedure for adding servers. FIRST_SERVER_IP should be the extra IP, if configured, of the first K3s server
----
FIRST_SERVER_IP=""
sudo K3S_TOKEN=<from the /var/lib/rancher/k3s/server/node-token file on the first server> INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_EXEC='server --server https://${FIRST_SERVER_IP}:6443 --write-kubeconfig-mode=644'  ./install.sh
----
////

endif::d1[]
endif::iRancher[]

ifdef::iRancher[]
ifdef::d1[]

==== {pn_Rancher} Deployment

{pn_Rancher} is a complete solution for managing Kubernetes clusters and Kubenetes applications. It addresses the operational and security challenges of managing multiple Kubernetes clusters and applications across any infrastructure. {pn_Rancher} streamlines Kubernetes cluster management on bare metal servers, private clouds, and vSphere environements; from the datacenter to the edge. {pn_Rancher} unites all of your Kubernetes clusters with global security policies, centralized authentication, access control and observability.

As {pn_Rancher} server is a native Kubernetes application, it will run on the single-node {pn_K3s} cluster. In instances where a load balancer is used to support the {pn_K3s} cluster, deploying two additional {pn_K3s} cluster nodes will automatically make {pn_Rancher} highly available. {pn_Rancher} uses the {pn_K3s} etcd key/value store to persist its data, which offers several advantages. Providing highly-available storage isn't needed to make {pn_Rancher} highly available. In addition, backing up the {pn_K3s} etcd store protects the cluster as well as the installation of {pn_Rancher}.

ifdef::iK3s[]
NOTE: These deployment steps are specific to {pn_K3s}. They can be executed from any host or node that has the kubectl tool and the KUBECONFIG file for the {pn_K3s} cluster.

The steps described here are for deploying {pn_Rancher} with self-signed security certificates. Other options are to have {pn_Rancher} create public certificates via Let's Encrypt (only with a publicly resolvable hostname for the {pn_Rancher} server) and to provide preconfigured, private certificates. See https://rancher.com/docs/rancher/v2.x/en/installation/install-rancher-on-k8s/#3-choose-your-ssl-configuration for more information.

.The primary steps for deploying {pn_Rancher} are:
1. Create the HelmChart custom resource for cert-manager
2. Create the HelmChart custom resource for {pn_Rancher}
3. Expose {pn_Rancher} through a Kubernetes NodePort service
4. (Optional) Create an SSH tunnel to access {pn_Rancher} in cases where the exposed server IP address and/or port is not accessible to the client browser
5. Connect to the {pn_Rancher} web UI

.Step 1: Create the HelmChart custom resource for cert-manager:
* At the time of writing, the most current, supported version of cert-manager is v1.0.4
* Set the following variable with the desired version of cert-manager
----
CERT_MANAGER_VERSION=""
----
* I.e. `CERT_MANAGER_VERSION="v1.0.4"`

* Create the cert-manager HelmChart custom resource manifest
----
cat <<EOF> cert-manager-helm-crd.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: kube-system
spec:
  chart: cert-manager
  targetNamespace: cert-manager
  version: ${CERT_MANAGER_VERSION}
  repo: https://charts.jetstack.io
EOF
----

* Create the cert-manager CRDs and apply the HelmChart resource manifest: 
----
kubectl create namespace cert-manager
//// kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/${CERT_MANAGER_VERSION}/cert-manager.crds.yaml ////
sudo wget -P /var/lib/rancher/k3s/server/manifests/ https://github.com/jetstack/cert-manager/releases/download/${CERT_MANAGER_VERSION}/cert-manager.crds.yaml
sudo mv cert-manager-helm-crd.yaml /var/lib/rancher/k3s/server/manifests/
----

* Monitor the progress of the installation: `watch -c "kubectl get pods -n cert-manager"`
** The installation is complete when all pods have a status of "Completed" or a status of "Running" with the number of "READY" pods being "1/1", "2/2", etc.
** Use Ctrl+c to exit the watch loop after all pods are running

.Step 2: Create the HelmChart custom resource for {pn_Rancher}:
* Set the following variable to the hostname of the {pn_Rancher} server instance
----
HOSTNAME=""
----
* I.e. `HOSTNAME="suse-rancher.sandbox.local"`

NOTE: This hostname should be resolvable to the primary or second IP address of the {K8s_name} host.

* Create the SUSE-Rancher HelmChart custom resource manifest
----
cat <<EOF> suse-rancher-helm-crd.yaml
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: rancher
  namespace: kube-system
spec:
  chart: rancher
  targetNamespace: cattle-system
  repo: https://releases.rancher.com/server-charts/stable
  set:
    hostname: ${HOSTNAME}
EOF
----

* Apply the HelmChart resource manifest: 
----
kubectl create namespace cattle-system
sudo mv suse-rancher-helm-crd.yaml /var/lib/rancher/k3s/server/manifests/
----

* Monitor the progress of the installation: `watch -c "kubectl get pods -n cattle-system"`
** The installation is complete when all pods have a status of "Completed" or a status of "Running" with the number of "READY" pods being "1/1", "2/2", etc.
** Use Ctrl+c to exit the watch loop after all pods are running

.Step 3: (Optional) Create an SSH tunnel to access {pn_Rancher}: 
 
 NOTE: This optional step is useful in cases where NAT routers and/or firewalls prevent the client web browser from reaching the exposed {pn_Rancher} server IP address and/or port. This step requires that a Linux host is accessible through SSH from the client system and that the Linux host can resolve and reach the exposed {pn_Rancher} service.

* Create an SSH tunnel through the Linux host to the IP address of the {pn_Rancher} server on the NodePort, as noted in Step 3:
----
ssh -N -D 8080 user@Linux-host
----
* On the local web browser, change the SOCKS Host settings to "127.0.0.1" and port "8080"

NOTE: This will route all traffic from this web browser through the remote Linux host. Be sure to close the tunnel and remvoe the SOCKS Host settings when you're done.


.Step 4: Connect to the {pn_Rancher} web UI and configure {pn_Rancher}:
* On the client system, use a web browser to connect to the {pn_Rancher} service
** I.e. `https://rancher.sandbox.local` 
* Provide a new Admin password
* IMPORTANT: On the second configuration page, ensure the "Rancher Server URL" is set to the hostname specified when creating the {pn_Rancher} HelmChart custom resource and the port is 443.

** I.e. `rancher.sandbox.local:443`

endif::iK3s[]

endif::d1[]
endif::iRancher[]

endif::SWDepCfg[]
