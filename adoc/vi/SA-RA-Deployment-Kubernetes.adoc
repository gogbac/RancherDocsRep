
ifdef::iK3s,iRKE1,iRKE2[]

==== Kubernetes Deployment

ifdef::iK3s[]

ifdef::iRancher[]
For this deployment, a single server installed with the {pn_SLEMicro} immutable operating system will support a single instance of {pn_K3s}. For maximum flexibility, {pn_K3s} will be deployed in a manner that would allow expanding the single-node cluster into a highly available, three-node Kubernetes cluster at a later date. 

While it is highly recommended that Kubernetes workloads (in this case the {pn_Rancher} ) be isolated from the Kubernetes control-plane and data-plane; this design will maintain all functions, including the {pn_Rancher}, on this server node. In this specialized case, the {pn_Rancher} workload is a known quantity and no other workloads will be run on this Kubernetes cluster. For this reason the {pn_Rancher} cluster is more closely aligned with appliance model best practices.

//-
Deployment Process::


The primary steps for deploying this single node {pn_K3s} cluster are:

. Find the appropriate version of the {pn_K3s} binary
* Verify the supported versions of {pn_K3s} at: {pn_Rancher_SupURL}, under the "Rancher Support Matrix"
* Set the following variable with the desired version of {pn_K3s}, exactly as shown in the Rancher Support Matrix:
+
----
K3s_VERSION=""
----
+
** e.g., `K3s_VERSION="v1.20.4+k3s1"`
+
. Install {pn_K3s} with embedded etcd enabled:
+
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} INSTALL_K3S_EXEC='server --cluster-init --write-kubeconfig-mode=644' sh -s -
----
+
* Monitor the progress of the installation: `watch -c "kubectl get deployments -A"`
** The deployment is complete when all deployments (coredns, local-path-provisioner, metrics-server, and traefik) show at least "1" as "AVAILABLE"
*** Use Ctrl+c to exit the watch loop after all pods are running

ifdef::BP[]
While a single {pn_K3s} node works perfectly fine, a fully HA {pn_K3s} cluster is highly recommended for production workloads. The etcd key/value store (aka database) requires an odd number of nodes be allocated to the {pn_K3s} plane (aka master nodes). In this case, two additional control-plane nodes will be added; for a total of three.

* Execute the following sets of commands on each of the remaining control-plane server nodes:
----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the first of the first server
----
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} K3S_URL=https://${FIRST_SERVER_IP}:6443 K3S_TOKEN=${NODE_TOKEN} K3S_KUBECONFIG_MODE="644" INSTALL_K3S_EXEC='server' sh -
----

By default, the {pn_K3s} server nodes are available to run non-control-plane workloads. This can be changed to the normal Kubernetes default by adding a taint to each server node. See the official Kubernetes documentation for more information on how to do that. In this case, the {pn_K3s} default behavior is perfect for the {pn_Rancher} server cluster as it doesn't require additional agent (aka worker) nodes to maintain a highly available {pn_Rancher} server application.

* In cases where agent nodes are desired, execute the following sets of commands on each of the agent nodes to add it to the {pn_K3s} cluster:
----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/k3s/server/node-token file on the first server
K3s_VERSION=""          # Match the first of the first server
----
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=${K3s_VERSION} K3S_URL=https://${FIRST_SERVER_IP}:6443 K3S_TOKEN=${NODE_TOKEN} K3S_KUBECONFIG_MODE="644" sh -
----
endif::BP[]

endif::iRancher[]

endif::iK3s[]

endif::iK3s,iRKE1,iRKE2[]

