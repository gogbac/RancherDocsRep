
ifdef::focusRKE2,layerRKE2[]
=== {pn_RKE2}

// leverage multiple prep sections
ifdef::layerSLEMicro,layerSLES[include::./SA-RA-Deployment-OS-prep.adoc[]]
ifdef::focusRancher[]
+
. Identify the appropriate, supported version of the {pn_RKE2} binary (e.g. vX.YY.ZZ+rke2s1), by reviewing the "{portfolioName} Support Matrix" on the {pn_Rancher_SupURL}[Support and Maintenance Terms of Service] web page. 
endif::focusRancher[]
ifndef::focusRancher[]
+
. Identify the appropriate, desired version of the {pn_RKE2} binary (e.g. vX.YY.ZZ+rke2s1), by reviewing the "Releases" on the {pn_RKE2_Download}[Download] web page. 
endif::focusRancher[]

//-
Deployment Process::
Perform the following steps to install the first {pn_RKE2} server on one of the nodes to be used for the Kubernetes control plane
// ifdef::focusRKE2[]
// ifdef::layerRKE2[]
// To meet the {pn_Rancher} prerequisites and requirements on supported Kubernetes instances,
// ifdef::layerRKE2[{pn_RKE2_ProductPage}[{pn_RKE2}]]
// can be utilized, and as desired later scaled out to a production cluster.
+
. Set the following variable with the noted version of {pn_RKE2}, as found during the preparation steps.
+
----
RKE2_VERSION=""
----
+
. Install the version of {pn_RKE2} with embedded etcd enabled:
+
----
curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=${RKE2_VERSION} INSTALL_RKE2_EXEC='server --cluster-init --write-kubeconfig-mode=644' sh -s -
----
+
ifdef::BP[]
TIP: To address <<G_Availability>> and possible <<G_Scaling>> to a multiple node cluster, etcd is enabled instead of using the default SQLite datastore.
+
endif::BP[]
** Monitor the progress of the installation: `watch -c "kubectl get deployments -A"`
*** The {pn_RKE2} deployment is complete when elements of all the deployments (coredns, local-path-provisioner, metrics-server, and traefik) show at least "1" as "AVAILABLE"
*** Use Ctrl+c to exit the watch loop after all deployment pods are running

ifdef::BP[]
//-
Deployment Consideration(s)::
To further optimize deployment factors, leverage the following practices:
ifdef::FCTR+Availability[]
* <<G_Availability>>
** While a single {pn_RKE2} node works perfectly fine, a full high-availability {pn_RKE2} cluster is recommended for production workloads. The etcd key/value store (aka database) requires an odd number of servers (aka master nodes) be allocated to the {pn_RKE2} cluster. In this case, two additional control-plane servers should be added; for a total of three.
+
. Deploy the same operating system on the new compute platform nodes, then log into the new nodes as root or as a user with sudo privileges.
. Execute the following sets of commands on each of the remaining control-plane nodes:
+
* Set the following variables, as appropriate for this cluster
----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/rke2/server/node-token file on the first server
RKE2_VERSION=""          # Match the first of the first server
----
+
* Install {pn_RKE2}
----
curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=${RKE2_VERSION} RKE2_URL=https://${FIRST_SERVER_IP}:6443 RKE2_TOKEN=${NODE_TOKEN} RKE2_KUBECONFIG_MODE="644" INSTALL_RKE2_EXEC='server' sh -
----

* Monitor the progress of the installation: `watch -c "kubectl get deployments -A"`
** The {pn_RKE2} deployment is complete when elements of all the deployments (coredns, local-path-provisioner, metrics-server, and traefik) show at least "1" as "AVAILABLE"
** Use Ctrl+c to exit the watch loop after all deployment pods are running

+
ifdef::focusRancher[]
By default, the {pn_RKE2} server nodes are available to run non-control-plane workloads. In this case, the {pn_RKE2} default behavior is perfect for the {pn_Rancher} server cluster as it doesn't require additional agent (aka worker) nodes to maintain a highly available {pn_Rancher} server application.
+
endif::focusRancher[]
NOTE: This can be changed to the normal Kubernetes default by adding a taint to each server node. See the official Kubernetes documentation for more information on how to do that.
+
. (Optional) In cases where agent nodes are desired, execute the following sets of commands on each of the agent nodes to add it to the {pn_RKE2} cluster:
+
----
FIRST_SERVER_IP=""      # Private IP preferred, if available
NODE_TOKEN=""           # From the /var/lib/rancher/rke2/server/node-token file on the first server
RKE2_VERSION=""          # Match the first of the first server
----
+
----
curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=${RKE2_VERSION} RKE2_URL=https://${FIRST_SERVER_IP}:6443 RKE2_TOKEN=${NODE_TOKEN} RKE2_KUBECONFIG_MODE="644" sh -
----
endif::FCTR+Availability[]
endif::BP[]

// endif::iRancher[]

ifdef::focusRKE2[]
// Next Steps::
After this successful deployment of the {pn_RKE2} solution, review the {pn_RKE2_DocURL}[product documentation] for details on how to directly utilize this Kubernetes cluster. Furthermore, by reviewing the {pn_Rancher} {pn_Rancher_DocURL}[product documentation] this solution can also be:

* imported ( refer to sub-section "Importing Existing Clusters" ), then
* managed ( refer to sub-section "Cluster Administration" ) and
* accessed ( refer to sub-section "Cluster Access" ) to address orchestration of workloads, maintaining security and many more functions are readily available.
endif::focusRKE2[]

endif::focusRKE2,layerRKE2[]

